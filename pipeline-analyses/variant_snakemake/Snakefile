ENV = "env.yml"
NBHD = ["hu-genome36", "hu-genome30", "hu-genome21", "hu-genome27"]
#NBHD = ["hu-genome21"]

rule all:
    input: 
        expand("outputs/{nbhd}/bcalm/reads-dedup.fa.unitigs.gfa", nbhd = NBHD)

rule download_plass:
    output: "inputs/plass/hu-s1-plass-hardtrim-clean-jan08.2019.tar.gz"
    shell:'''
    curl -L -o {output} https://osf.io/9hg85/download
    '''

rule decompress_plass:
    output: "inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup"
    input: "inputs/plass/hu-s1-plass-hardtrim-clean-jan08.2019.tar.gz"
    params: 
        folder = "inputs/plass",
        out = "hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup"
    shell:'''
    tar xvf {input} -C {params.folder} {params.out}
    '''

rule download_reads:
    output: "inputs/reads/hu-s1-reads-hardtrim-jan08.2019.tar.gz"
    shell:'''
    curl -L -o {output} https://osf.io/x5ezk/download 
    '''

rule decompress_reads:
    output: "inputs/reads/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz"
    input: "inputs/reads/hu-s1-reads-hardtrim-jan08.2019.tar.gz"
    params: 
        folder = "inputs/reads",
        out = "hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz"
    shell:'''
    tar xvf {input} -C {params.folder} {params.out}
    '''

rule download_pfam:
    output: "inputs/pfam/PF00521_gyra.sto"
    params: 
        out_dir = "inputs/pfam"
    shell:'''
    pfam=$(echo PF00521)
    wget -O {output} https://pfam.xfam.org/family/${{pfam}}/alignment/full
    '''

rule hmmbuild:
    input: "inputs/pfam/PF00521_gyra.sto"
    output: "outputs/hmmbuild/PF00521_gyra.hmm"
    conda: ENV
    shell: '''
    hmmbuild {output} {input}   
    hmmpress {output}
'''

rule hmmscan:
    output:
        out = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100.out",
        tbl = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100-tbl.out",
        dom = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100-dom.out"
    input:
        hmm = "outputs/hmmbuild/PF00521_gyra.hmm",
        faa = "inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup"
    conda: ENV
    shell:'''
    hmmscan -T 100 -o {output.out} --tblout {output.tbl} --domtblout {output.dom} {input.hmm} {input.faa} 
'''

rule def_hmm_matches_and_find_reads:
# get the names of the PLASS assembled aa seqs that match the PFAM
# domain of interest.
    input:
        dom = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100-dom.out",
        rhmmer = "rhmmer.R"
    output:
        keep = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100-NAMES.txt"
    conda: ENV
    script:'variant-workflow-hmm-matches.R'

rule paladin_index:
    output: "inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup.bwt"
    input: "inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup"
    conda: ENV
    shell:'''
    paladin index -r3 {input}
    '''

rule paladin_align:
    output: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sam"
    input:
        indx="inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup.bwt",
        reads="inputs/reads/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz"
    params: 
        indx="inputs/plass/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz.plass.cdhit.fa.clean.cut.dup"
    conda: ENV
    shell:'''
    paladin align -f 125 -t 2 {params.indx} {input.reads} > {output}
    '''

rule samtools_flagstat_paladin:
    output: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sam.flagstat"
    input: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sam"
    conda: ENV
    shell:'''
    samtools flagstat {input} > {output}
    '''

rule samtools_view_paladin:
    output: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.bam"
    input: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sam"
    conda: ENV
    shell:'''
    samtools view -b {input} > {output}
    '''

rule samtools_sort_paladin:
    output: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sort.bam"
    input: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.bam"
    conda: ENV
    shell:'''
    samtools sort {input} > {output}
    '''

rule samtools_index_paladin:
    output: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sort.bam.bai"
    input: "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sort.bam"
    conda: ENV
    shell:'''
    samtools index {input} 
    '''

rule extract_plass_readnames_from_bam:
# extract names of reads for each aa seq bam section that matched 
# PFAM domain of interest
    output: dynamic("outputs/{nbhd}/bam_subsets/{plass_match}-NAMES.txt") 
    input:
        plass_names = "outputs/{nbhd}/hmmscan/{nbhd}.hardtrim.plass.PF00521_gyra-hmmscanT100-NAMES.txt", 
        bai = "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sort.bam.bai", 
        bam = "outputs/{nbhd}/paladin/{nbhd}.hardtrim.sort.bam" 
    run:
        import pysam
        import re
        
        with open(str(input.plass_names)) as f:
            plass_hmmscan_matches = f.readlines()

        plass_hmmscan_matches = [x.strip() for x in plass_hmmscan_matches] 
        
        nbhd_folder = str(input.bai)
        nbhd_folder = re.sub('outputs/', '', nbhd_folder)
        nbhd_folder = re.sub('/paladin/.*', '', nbhd_folder) 
        print(nbhd_folder)
        samfile = pysam.AlignmentFile(str(input.bam), 'rb')

        for match in plass_hmmscan_matches:
            reads = []
            for read in samfile.fetch(match):
                name = re.sub('.*:', '', read.qname)
                reads.append(name)
                with open("outputs/" + nbhd_folder + f"/bam_subsets/{match}-NAMES.txt", 'w') as outfile:
                    for s in reads:
                        outfile.write("%s\n" % s)
        samfile.close()


rule grab_plass_reads:
    output: "outputs/{nbhd}/bam_subsets/{plass_match}.fa" # loses fq info
    input: 
        names = "outputs/{nbhd}/bam_subsets/{plass_match}-NAMES.txt",
        reads= "inputs/reads/hu-s1_k31_r1_search_oh0/{nbhd}.fa.cdbg_ids.reads.hardtrim.fa.gz"
    conda: ENV
    shell:'''
    ./extract-hmmscan-matches.py {input.names} {input.reads}  > {output} 
    '''

rule concatenate_reads:
    output: "outputs/{nbhd}/bcalm/reads.fa"
    input: dynamic("outputs/{nbhd}/bam_subsets/{plass_match}.fa")
    shell:'''
    cat {input} > {output}
    '''

rule deduplicate_reads:
    output: "outputs/{nbhd}/bcalm/reads-dedup.fa"
    input: "outputs/{nbhd}/bcalm/reads.fa"
    conda: ENV
    shell:'''
    fastx_collapser < {input} > {output}
    ''' 

rule bcalm_reads:
    output: "outputs/{nbhd}/bcalm/reads-dedup.fa.unitigs.fa"
    input: "outputs/{nbhd}/bcalm/reads-dedup.fa"
    params: out_dir = "outputs/{nbhd}/bcalm"
    conda: ENV
    shell:'''
    bcalm -in {input} -kmer-size 31 -out {input} -out-dir {params.out_dir}
    '''

rule unitigs_to_gfa:
    input: "outputs/{nbhd}/bcalm/reads-dedup.fa.unitigs.fa"
    output: "outputs/{nbhd}/bcalm/reads-dedup.fa.unitigs.gfa"
    conda: ENV
    shell:'''
    python convertToGFA.py {input} {output} 31
    '''
